{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZFoTZ9Rd4bP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbamman/nlp22/blob/master/HW2/HW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TQTT9x-6d2JI"
      },
      "outputs": [],
      "source": [
        "import sys, argparse\n",
        "from scipy import sparse\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import operator\n",
        "import nltk\n",
        "import csv\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from pandas import option_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e4KuVSCSqlUX",
        "outputId": "a25550f0-7c0d-4adc-e819-929ba88e5273",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/python3.7/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
            "  warn(RuntimeWarning(msg))\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "!python -m nltk.downloader punkt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hk07KCgwoZy"
      },
      "source": [
        "Let's download the data we'll use for training and development, and also the data we'll use to make predictions for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hn0XtfFeqP2P",
        "outputId": "35fbc859-2b2d-4990-f36d-b5e0bbd529e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-03 00:13:35--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1322055 (1.3M) [text/plain]\n",
            "Saving to: ‘train.txt’\n",
            "\n",
            "train.txt           100%[===================>]   1.26M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-02-03 00:13:35 (19.3 MB/s) - ‘train.txt’ saved [1322055/1322055]\n",
            "\n",
            "--2022-02-03 00:13:35--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/dev.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1309909 (1.2M) [text/plain]\n",
            "Saving to: ‘dev.txt’\n",
            "\n",
            "dev.txt             100%[===================>]   1.25M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2022-02-03 00:13:36 (18.7 MB/s) - ‘dev.txt’ saved [1309909/1309909]\n",
            "\n",
            "--2022-02-03 00:13:36--  https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6573426 (6.3M) [text/plain]\n",
            "Saving to: ‘test.txt’\n",
            "\n",
            "test.txt            100%[===================>]   6.27M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-03 00:13:37 (64.1 MB/s) - ‘test.txt’ saved [6573426/6573426]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get data\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/train.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/dev.txt\n",
        "!wget https://raw.githubusercontent.com/dbamman/nlp22/main/HW2/test.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jq2yq0xpRCUb"
      },
      "outputs": [],
      "source": [
        "trainingFile = \"train.txt\"\n",
        "evaluationFile = \"dev.txt\"\n",
        "testFile = \"test.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CGiM8qQiJOBU"
      },
      "outputs": [],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code.\n",
        "## This defines the classification class which\n",
        "## loads the data and sets up the model.\n",
        "######################################################################\n",
        "\n",
        "class Classifier:\n",
        "\n",
        "    def __init__(self, feature_method, L2_regularization_strength=1.0, min_feature_count=1):\n",
        "        self.feature_vocab = {}\n",
        "        self.feature_method = feature_method\n",
        "        self.log_reg = None\n",
        "        self.L2_regularization_strength=L2_regularization_strength\n",
        "        self.min_feature_count=min_feature_count\n",
        "\n",
        "        self.trainX, self.trainY, self.trainOrig = self.process(trainingFile, training=True)\n",
        "        self.devX, self.devY, self.devOrig = self.process(evaluationFile, training=False)\n",
        "        self.testX, _, self.testOrig = self.process(testFile, training=False)\n",
        "\n",
        "    # Read data from file\n",
        "    def load_data(self, filename):\n",
        "        data = []\n",
        "        with open(filename, encoding=\"utf8\") as file:\n",
        "            for line in file:\n",
        "                cols = line.split(\"\\t\")\n",
        "                idd = cols[0]\n",
        "                label = cols[1]\n",
        "                text = cols[2]\n",
        "\n",
        "                data.append((idd, label, text))\n",
        "                \n",
        "        return data\n",
        "\n",
        "    # Featurize entire dataset\n",
        "    def featurize(self, data):\n",
        "        featurized_data = []\n",
        "        for idd, label, text in data:\n",
        "            feats = self.feature_method(text)\n",
        "            featurized_data.append((label, feats))\n",
        "        return featurized_data\n",
        "\n",
        "    # Read dataset and returned featurized representation as sparse matrix + label array\n",
        "    def process(self, dataFile, training = False):\n",
        "        original_data = self.load_data(dataFile)\n",
        "        data = self.featurize(original_data)\n",
        "\n",
        "        if training:\n",
        "            fid = 0\n",
        "            feature_doc_count = Counter()\n",
        "            for label, feats in data:\n",
        "                for feat in feats:\n",
        "                    feature_doc_count[feat]+= 1\n",
        "\n",
        "            for feat in feature_doc_count:\n",
        "                if feature_doc_count[feat] >= self.min_feature_count:\n",
        "                    self.feature_vocab[feat] = fid\n",
        "                    fid += 1\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (label, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = label\n",
        "\n",
        "        return X, Y, original_data\n",
        "\n",
        "    def load_test(self, dataFile):\n",
        "        data = self.load_data(dataFile)\n",
        "        data = self.featurize(data)\n",
        "\n",
        "        F = len(self.feature_vocab)\n",
        "        D = len(data)\n",
        "        X = sparse.dok_matrix((D, F))\n",
        "        Y = [None]*D\n",
        "        for idx, (data_id, feats) in enumerate(data):\n",
        "            for feat in feats:\n",
        "                if feat in self.feature_vocab:\n",
        "                    X[idx, self.feature_vocab[feat]] = feats[feat]\n",
        "            Y[idx] = data_id\n",
        "\n",
        "        return X, Y\n",
        "\n",
        "    # Train model and evaluate on held-out data\n",
        "    def evaluate(self):\n",
        "        (D,F) = self.trainX.shape\n",
        "        self.log_reg = linear_model.LogisticRegression(C = self.L2_regularization_strength, max_iter=1000)\n",
        "        self.log_reg.fit(self.trainX, self.trainY)\n",
        "        training_accuracy = self.log_reg.score(self.trainX, self.trainY)\n",
        "        development_accuracy = self.log_reg.score(self.devX, self.devY)\n",
        "        print(\"Method: %s, Features: %s, Train accuracy: %.3f, Dev accuracy: %.3f\" % (self.feature_method.__name__, F, training_accuracy, development_accuracy))\n",
        "\n",
        "\n",
        "    # Predict labels for new data\n",
        "    def predict(self):\n",
        "        predX = self.log_reg.predict(self.testX)\n",
        "\n",
        "        with open(\"%s_%s\" % (self.feature_method.__name__, \"predictions.csv\"), \"w\", encoding=\"utf8\") as out:\n",
        "            writer=csv.writer(out)\n",
        "            writer.writerow([\"Id\", \"Expected\"])\n",
        "            for idx, data_id in enumerate(self.testX):\n",
        "                writer.writerow([self.testOrig[idx][0], predX[idx]])\n",
        "        out.close()\n",
        "\n",
        "\n",
        "    def printWeights(self, n=10):\n",
        "\n",
        "        reverse_vocab=[None]*len(self.log_reg.coef_[0])\n",
        "        for k in self.feature_vocab:\n",
        "            reverse_vocab[self.feature_vocab[k]]=k\n",
        "\n",
        "        # binary\n",
        "        if len(self.log_reg.classes_) == 2:\n",
        "              weights=self.log_reg.coef_[0]\n",
        "\n",
        "              cat=self.log_reg.classes_[1]\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "              cat=self.log_reg.classes_[0]\n",
        "              for feature, weight in list(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1)))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "        # multiclass\n",
        "        else:\n",
        "          for i, cat in enumerate(self.log_reg.classes_):\n",
        "\n",
        "              weights=self.log_reg.coef_[i]\n",
        "\n",
        "              for feature, weight in list(reversed(sorted(zip(reverse_vocab, weights), key = operator.itemgetter(1))))[:n]:\n",
        "                  print(\"%s\\t%.3f\\t%s\" % (cat, weight, feature))\n",
        "              print()\n",
        "\n",
        "            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDmfkG782kgo"
      },
      "source": [
        "*First*, let's define a classifier based on a really simple dictionary-based feature: if the abstract contains the words \"love\" or \"like\", the CONTAINS_POSITIVE_WORD feature will fire, and if it contains either \"hate\" or \"dislike\", the CONTAINS_NEGATIVE_WORD will fire.  Note how we use `nltk.word_tokenize` to tokenize the text into its discrete words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xCq1bL3e2jUj"
      },
      "outputs": [],
      "source": [
        "def simple_featurize(text):\n",
        "    feats = {}\n",
        "    words = nltk.word_tokenize(text)\n",
        "\n",
        "    for word in words:\n",
        "        word=word.lower()\n",
        "        if word == \"love\" or word == \"like\":\n",
        "            feats[\"contains_positive_word\"] = 1\n",
        "        if word == \"hate\" or word == \"dislike\":\n",
        "            feats[\"contains_negative_word\"] = 1\n",
        "            \n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3PQdN9r3Ujz"
      },
      "source": [
        "Now let's see how that feature performs on the development data.  Note the `L2_regularization_strength` specifies the strength of the L2 regularizer (values closer to 0 = stronger regularization), and the `min_feature_count` specifies how many data points need to contain a feature for it to be allowable as a feature in the model.  Both are ways to prevent the model from overfitting and achieve higher performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Jnqjxd6fKPiP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd078b68-06ce-467e-9dfe-8a51f4035608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: simple_featurize, Features: 2, Train accuracy: 0.509, Dev accuracy: 0.500\n"
          ]
        }
      ],
      "source": [
        "simple_classifier = Classifier(simple_featurize, L2_regularization_strength=1.0, min_feature_count=1)\n",
        "simple_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO4XQzU3PdeU"
      },
      "source": [
        "First, is this accuracy score any good?  Let's calculate the accuracy of a majority class predictor to provide some context.  Again, this determines the most represented (majority) class in the training data, and then predicts every test point to be this class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8t--LfOjPj7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877899a2-e45e-4662-98bf-349c3af912f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Majority class: pos\tDev accuracy: 0.500\n"
          ]
        }
      ],
      "source": [
        "def majority_class(trainY, devY):\n",
        "    labelCounts=Counter()\n",
        "    for label in trainY:\n",
        "        labelCounts[label]+=1\n",
        "    majority_class=labelCounts.most_common(1)[0][0]\n",
        "    \n",
        "    correct=0.\n",
        "    for label in devY:\n",
        "        if label == majority_class:\n",
        "            correct+=1\n",
        "            \n",
        "    print(\"Majority class: %s\\tDev accuracy: %.3f\" % (majority_class, correct/len(devY)))\n",
        "majority_class(simple_classifier.trainY, simple_classifier.devY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIEkYOWO5ClC"
      },
      "source": [
        "# Your assignment\n",
        "\n",
        "## Deliverable 1\n",
        "\n",
        "Your job in this homework is to implement a binary bag-of-words model (i.e., one that assigns a feature value of 1 to each word type that is present in the text); and to brainstorm three additional distinct classes of features, justify why they might help improve the performance *over a bag of words* for this task, implement them in code, and then assess their independent performance on the development data. \n",
        "\n",
        "Describe your features and report their performance in the table below; implement the features in the specified `feature1`, `feature2`, and `feature3` functions, and execute each respective classifier to show its performance.  \n",
        "\n",
        "|Feature|Why should it work? (50 words each)|Best dev set performance|Train set performance|L2|\n",
        "|---|---|---|---|---|\n",
        "|Bag of words||0.789|0.982|3e-2\n",
        "|Absence-inverted AFINN scores|This feature is stronger than just the presence/absence of words like in BOW. If a negative word isn't present, it gets a strong positive weight and vice versa.|0.778|0.965|1e-1\n",
        "|Bigrams|Bigrams convey more positional information than unigrams used in BOW.|0.755|1.000|1e0\n",
        "|Sinusoidal position embeddings|Words closer together have a closer value, allowing the model to gain more structural information.|0.770|0.936|1e-2\n",
        "|Overall||0.823|0.988|6e-3\n",
        "\n",
        "Note that it is not required for your features to actually perform well, but your justification for why it *should* perform better than a bag of words should be defensible.  The most creative features (defined as features that few other students use and that are reasonably well-performing) will receive extra credit for this assignment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vVl1zAREekC3"
      },
      "outputs": [],
      "source": [
        "def bag_of_words(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    return {t: 1 for t in set(nltk.word_tokenize(text.lower()))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_3AJ5qMBeqmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d433ebe4-95ca-43c2-9450-fd12f012a6be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: bag_of_words, Features: 21254, Train accuracy: 0.982, Dev accuracy: 0.789\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "bow_classifier = Classifier(bag_of_words, L2_regularization_strength=3e-2, min_feature_count=1)\n",
        "bow_classifier.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/fnielsen/afinn/master/afinn/data/AFINN-en-165.txt\n",
        "afinn = dict(map(lambda p: (p[0], int(p[1])), [line.split('\\t') for line in open(\"AFINN-en-165.txt\")]))\n",
        "afinn_negative = {f\"AFINN_{k}\": -v for k, v in afinn.items()}"
      ],
      "metadata": {
        "id": "_UBZFCgu45te"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ocPMYhIt4BX0"
      },
      "outputs": [],
      "source": [
        "def feature1(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "    tokens = set(nltk.word_tokenize(text.lower()))\n",
        "\n",
        "    feats = afinn_negative.copy()\n",
        "    for t in tokens:\n",
        "      t = f\"AFINN_{t}\"\n",
        "      if t in afinn_negative:\n",
        "        feats[t] *= -1\n",
        "\n",
        "    return feats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-MAwRwbQ7lVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f9c017-9892-4d56-9b58-3e66034c94f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature1, Features: 3382, Train accuracy: 0.981, Dev accuracy: 0.773\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "classifier1 = Classifier(feature1, L2_regularization_strength=1e-1, min_feature_count=1)\n",
        "classifier1.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LNlQyjEB4Bwt"
      },
      "outputs": [],
      "source": [
        "def feature2(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    bigrams = nltk.bigrams(tokens)\n",
        "    return {b: 1 for b in bigrams}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "JgpuykF67oWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe68bbe-4751-4ac6-933c-5fb1b79299fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature2, Features: 127808, Train accuracy: 1.000, Dev accuracy: 0.755\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "classifier2 = Classifier(feature2, L2_regularization_strength=1e0, min_feature_count=1)\n",
        "classifier2.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FmJKucgn4CEg"
      },
      "outputs": [],
      "source": [
        "def feature3(text):\n",
        "    # Here the `feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    return {f\"POSITION_EMBEDDING_{t}\": np.sin((np.pi / 2) + np.pi * (i / 2000)) for i, t in enumerate(tokens)}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "g_f--utb7q4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeb92a2e-89ab-4a1d-8b10-dce4ab3a676b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: feature3, Features: 21254, Train accuracy: 0.936, Dev accuracy: 0.770\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "classifier3 = Classifier(feature3, L2_regularization_strength=1e-2, min_feature_count=1)\n",
        "classifier3.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEpK5LyMgv5c"
      },
      "source": [
        "Next, let's combine any or all the features you have developed into one big model and make predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jxKmEqI5JY71"
      },
      "outputs": [],
      "source": [
        "def combiner_function(text):\n",
        "\n",
        "    # Here the `all_feats` dict should contain the features -- the key should be the feature name, \n",
        "    # and the value is the feature value.  See `simple_featurize` for an example.\n",
        "    \n",
        "  all_feats={}\n",
        "  for feature in [bag_of_words, feature1, feature2, feature3]:\n",
        "    all_feats.update(feature(text))\n",
        "  return all_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "D-tRUFTIdAqT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0b91868-675a-4a93-9fea-048f0e17db45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Method: combiner_function, Features: 173698, Train accuracy: 0.988, Dev accuracy: 0.823\n"
          ]
        }
      ],
      "source": [
        "######################################################################\n",
        "## Do not edit this block of code, except for the L2_regularization_strength and min_feature_count parameters\n",
        "######################################################################\n",
        "big_classifier = Classifier(combiner_function, L2_regularization_strength=6e-3, min_feature_count=1)\n",
        "big_classifier.evaluate()\n",
        "\n",
        "#generate .csv file with prediction output on test data\n",
        "big_classifier.predict()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg2J1BLgatMP"
      },
      "source": [
        " ## Deliverable 2\n",
        "\n",
        "This code will generate a file named `combiner_function_predictions.csv`; download this file (using e.g. the file manager on the left panel in Colab) and submit this to GradeScope along with your notebook; the 5 systems with the highest performance (revealed after the submission deadline) will receive extra credit for this assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lgyoJm09pqe"
      },
      "source": [
        "## Interrogating classifiers\n",
        "\n",
        "Below you will find several ways in which you can interrogate your model to get ideas on ways to improve its performance.  **Note that nothing below this line requires any work on your part; treat these as useful tools for understanding what works and what doesn't.**\n",
        "\n",
        "1. First, let's look at the confusion matrix of its predictions (where we can compare the true labels with the predicted labels).  What kinds of mistakes is it making?  (While this is mainly helpful in the context of multiclass classification, we can still see if there's a bias toward predicting a specific class in the binary setting as well). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "7ulxd1TosIMV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "outputId": "46f5f2f3-8ef1-4693-edeb-21b03515f508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "def print_confusion(classifier):\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "    plot_confusion_matrix(classifier.log_reg, classifier.devX, classifier.devY, ax=ax, xticks_rotation=\"vertical\", values_format=\"d\")\n",
        "    plt.show()\n",
        "\n",
        "print_confusion(big_classifier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPhH4flIuEbx"
      },
      "source": [
        "2. Next, let's look at the features that are most defining for each of the classes (ranked by how strong their corresponding coefficient is).  Do the features you are defining help in the ways you think they should?  Do sets of successful features suggests others, or complementary features that may provide a different view on the data?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "IAyGuXIi9pqe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0809abd0-fa44-4fb4-f588-6b5193375313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pos\t0.148\tAFINN_worst\n",
            "pos\t0.129\tAFINN_great\n",
            "pos\t0.115\tAFINN_boring\n",
            "pos\t0.109\tAFINN_excellent\n",
            "pos\t0.103\tAFINN_horrible\n",
            "pos\t0.089\tAFINN_stupid\n",
            "pos\t0.088\tAFINN_amazing\n",
            "pos\t0.088\tAFINN_awful\n",
            "pos\t0.086\tAFINN_poor\n",
            "pos\t0.081\tAFINN_enjoyed\n",
            "pos\t0.080\tAFINN_best\n",
            "pos\t0.078\tAFINN_loved\n",
            "pos\t0.077\tAFINN_no\n",
            "pos\t0.076\tAFINN_poorly\n",
            "pos\t0.076\tAFINN_perfect\n",
            "pos\t0.076\tAFINN_worse\n",
            "pos\t0.075\tAFINN_bad\n",
            "pos\t0.071\tAFINN_wonderful\n",
            "pos\t0.070\tAFINN_beautifully\n",
            "pos\t0.068\tAFINN_top\n",
            "pos\t0.067\t('.', 'It')\n",
            "pos\t0.065\tAFINN_waste\n",
            "pos\t0.064\tAFINN_ridiculous\n",
            "pos\t0.064\tAFINN_fails\n",
            "pos\t0.062\tAFINN_wonderfully\n",
            "\n",
            "neg\t-0.069\tAFINN_violence\n",
            "neg\t-0.058\t?\n",
            "neg\t-0.053\tPOSITION_EMBEDDING_?\n",
            "neg\t-0.050\tacting\n",
            "neg\t-0.050\twould\n",
            "neg\t-0.049\tAFINN_brutal\n",
            "neg\t-0.049\tPOSITION_EMBEDDING_acting\n",
            "neg\t-0.048\t(',', 'I')\n",
            "neg\t-0.048\tPOSITION_EMBEDDING_would\n",
            "neg\t-0.046\tPOSITION_EMBEDDING_even\n",
            "neg\t-0.045\tAFINN_violent\n",
            "neg\t-0.045\tn't\n",
            "neg\t-0.043\tPOSITION_EMBEDDING_n't\n",
            "neg\t-0.043\tcould\n",
            "neg\t-0.043\teven\n",
            "neg\t-0.042\tAFINN_war\n",
            "neg\t-0.041\tAFINN_dead\n",
            "neg\t-0.040\tnothing\n",
            "neg\t-0.039\tPOSITION_EMBEDDING_were\n",
            "neg\t-0.039\twere\n",
            "neg\t-0.039\tPOSITION_EMBEDDING_nothing\n",
            "neg\t-0.038\tno\n",
            "neg\t-0.038\tPOSITION_EMBEDDING_could\n",
            "neg\t-0.037\tPOSITION_EMBEDDING_no\n",
            "neg\t-0.035\thad\n",
            "\n"
          ]
        }
      ],
      "source": [
        "big_classifier.printWeights(n=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e80DUsSXu7h9"
      },
      "source": [
        "3. Next, let's look at the individual data points that are most mistaken. Does it suggest any features you might create to disentangle them?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "I4uTzwV99pqe"
      },
      "outputs": [],
      "source": [
        "def analyze(classifier):\n",
        "    \n",
        "    probs=classifier.log_reg.predict_proba(classifier.devX)\n",
        "    predicts=classifier.log_reg.predict(classifier.devX)\n",
        "\n",
        "    classes={}\n",
        "    for idx, lab in enumerate(classifier.log_reg.classes_):\n",
        "        classes[lab]=idx\n",
        "\n",
        "    mistakes={}\n",
        "    for i in range(len(probs)):\n",
        "        if predicts[i] != classifier.devY[i]:\n",
        "            predicted_lab_idx=classes[predicts[i]]\n",
        "            mistakes[i]=probs[i][predicted_lab_idx]\n",
        "\n",
        "    frame=[]\n",
        "    sorted_x = sorted(mistakes.items(), key=operator.itemgetter(1), reverse=True)\n",
        "    for k, v in sorted_x:\n",
        "        idd=classifier.devOrig[k][0]\n",
        "        text=classifier.devOrig[k][2]\n",
        "        frame.append([idd, v, classifier.devY[k], predicts[k], text])\n",
        "\n",
        "    df=pd.DataFrame(frame, columns=[\"id\", \"P(predicted class confidence)\", \"Human label\", \"Prediction\", \"Text\"])\n",
        "\n",
        "    with option_context('display.max_colwidth', 200):\n",
        "        display(df.head(n=20))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "UXmRhSuzxaJi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5a107115-4cbc-463d-f029-1355e39c4a80"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cdf10add-e29d-4255-a925-60bf2fff81c0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>P(predicted class confidence)</th>\n",
              "      <th>Human label</th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1436</td>\n",
              "      <td>0.983385</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1004</td>\n",
              "      <td>0.947845</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>Certainly NOMAD has some of the best horse riding scenes, swordplay, and scrumptious landscape cinematography you'll likely see, but this isn't what makes a film good. It helps but the story has t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1670</td>\n",
              "      <td>0.905078</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I've always liked Fred MacMurray, andalthough her career was tragically cut shortI think Carole Lombard is fun to watch. Pair these two major and attractive stars together, add top supporting pl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1456</td>\n",
              "      <td>0.900172</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Hardly a masterpiece. Not so well written. Beautiful cinematography i think not. This movie wasn't too terrible but it wasn't that much better than average. The main story dealing with highly immo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1959</td>\n",
              "      <td>0.888030</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>This is one of the best episode from the second season of MOH, I think Mick Garris has a problem with women... He kill'em all, they are often the victims (Screwfly solution, Pro-life, Valerie on t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1822</td>\n",
              "      <td>0.884434</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a seque...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1608</td>\n",
              "      <td>0.865752</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>As good an advert for republicanism as you're ever likely to see,\"Mayerling\"is an everyday story of royal folk in late nineteenth century Austria.Set during one of Europe's seemingly incessant int...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1261</td>\n",
              "      <td>0.842752</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>NATURAL BORN KILLERS (1994)Cinema Cut: R Director's Cut: NC-17It's an unusual Oliver Stone picture, but when I read he was on drugs during the filming, I needed no further explanation. 'Natural Bo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1825</td>\n",
              "      <td>0.826533</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Whilst reading through the comments left for this show, I couldn't help but notice that a large percentage of the reviewers had either not actually watched any episodes of the show either all the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1659</td>\n",
              "      <td>0.826348</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>Child 'Sexploitation' is one of the most serious issues facing our world today and I feared that any film on the topic would jump straight to scenes of an explicitly sexual nature in order to shoc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1139</td>\n",
              "      <td>0.825430</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>I saw this movie as a kid on Creature Feature when I lived in New York. It was a pretty creepy movie, though not as good as Horror Hotel. I just bought this movie on DVD, and it is different from ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1913</td>\n",
              "      <td>0.825428</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I have only seen Gretchen Mol in two other films (Girl 6, Donnie Brasco), and don't really remember her, but she did a great job as a naive girl who posed for pictures because it made people happy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1862</td>\n",
              "      <td>0.817110</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>In the previews, \"The 40 Year-Old Virgin\" boasts the image of another immature sex romp about a 40-ish Lonely Guy who suddenly feels the urge to do the deed simply because he hasn't. Too many past...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1352</td>\n",
              "      <td>0.816136</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I was unsure whether or not Andy Sidaris could repeat his success with the cinematic hit \"Malibu Express.\" With his film Fit to Kill he has proved that Sidaris is a serious filmmaker and not just ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1317</td>\n",
              "      <td>0.812553</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>if.... is the cinematic equivalent of Sgt. Pepper's: Revered by baby boomers as the pinnacle of creation, and viewed as rather a silly bit of business by preceding and subsequent generations. Now ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1218</td>\n",
              "      <td>0.809579</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>This early B entry into the patriotic category slapped a gorgeous young Gene Tierney on the ads and posters, but you have to wait a good time before you glimpse her, riding in a Hollywoodized came...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1710</td>\n",
              "      <td>0.804175</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>I think Phillip Kaufman read the cliff's Notes version of the Kundera novel and then set about making this film. Okay, of course it won't have the punch of the original. Kundera's novels are great...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1787</td>\n",
              "      <td>0.802600</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>My roommate had bought this documentary and invited me to watch it with her. She's from China and only heard so much about 9/11 and wanted to know the cold hard truth and she wanted me to tell her...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1995</td>\n",
              "      <td>0.798543</td>\n",
              "      <td>neg</td>\n",
              "      <td>pos</td>\n",
              "      <td>'Maladolescenza' has the air of a dark fairy tale, with its child protagonists, forest setting, and the discovery of a castle's ruins. Yet at its core, the film is essentially an unusual psychosex...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1471</td>\n",
              "      <td>0.798429</td>\n",
              "      <td>pos</td>\n",
              "      <td>neg</td>\n",
              "      <td>But the opposite, sorry bud, i completely understand how you can be dragged into a film because you relate to the subject ( and you have). This film is terrible, the main character would give any ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cdf10add-e29d-4255-a925-60bf2fff81c0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cdf10add-e29d-4255-a925-60bf2fff81c0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cdf10add-e29d-4255-a925-60bf2fff81c0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      id  ...                                                                                                                                                                                                     Text\n",
              "0   1436  ...  I have never seen such a movie before. I was on the edge of my seat and constantly laughing throughout the entire movie. I never thought such horrible acting existed it was all just too funny. The...\n",
              "1   1004  ...  Certainly NOMAD has some of the best horse riding scenes, swordplay, and scrumptious landscape cinematography you'll likely see, but this isn't what makes a film good. It helps but the story has t...\n",
              "2   1670  ...  I've always liked Fred MacMurray, andalthough her career was tragically cut shortI think Carole Lombard is fun to watch. Pair these two major and attractive stars together, add top supporting pl...\n",
              "3   1456  ...  Hardly a masterpiece. Not so well written. Beautiful cinematography i think not. This movie wasn't too terrible but it wasn't that much better than average. The main story dealing with highly immo...\n",
              "4   1959  ...  This is one of the best episode from the second season of MOH, I think Mick Garris has a problem with women... He kill'em all, they are often the victims (Screwfly solution, Pro-life, Valerie on t...\n",
              "5   1822  ...  Zombi 3 has an interesting history in it's making. Firstly, it is a sequel to Fulci's hit Zombi 2, with Zombi 2 itself being of course a marketing ploy to trick people into thinking it was a seque...\n",
              "6   1608  ...  As good an advert for republicanism as you're ever likely to see,\"Mayerling\"is an everyday story of royal folk in late nineteenth century Austria.Set during one of Europe's seemingly incessant int...\n",
              "7   1261  ...  NATURAL BORN KILLERS (1994)Cinema Cut: R Director's Cut: NC-17It's an unusual Oliver Stone picture, but when I read he was on drugs during the filming, I needed no further explanation. 'Natural Bo...\n",
              "8   1825  ...  Whilst reading through the comments left for this show, I couldn't help but notice that a large percentage of the reviewers had either not actually watched any episodes of the show either all the ...\n",
              "9   1659  ...  Child 'Sexploitation' is one of the most serious issues facing our world today and I feared that any film on the topic would jump straight to scenes of an explicitly sexual nature in order to shoc...\n",
              "10  1139  ...  I saw this movie as a kid on Creature Feature when I lived in New York. It was a pretty creepy movie, though not as good as Horror Hotel. I just bought this movie on DVD, and it is different from ...\n",
              "11  1913  ...  I have only seen Gretchen Mol in two other films (Girl 6, Donnie Brasco), and don't really remember her, but she did a great job as a naive girl who posed for pictures because it made people happy...\n",
              "12  1862  ...  In the previews, \"The 40 Year-Old Virgin\" boasts the image of another immature sex romp about a 40-ish Lonely Guy who suddenly feels the urge to do the deed simply because he hasn't. Too many past...\n",
              "13  1352  ...  I was unsure whether or not Andy Sidaris could repeat his success with the cinematic hit \"Malibu Express.\" With his film Fit to Kill he has proved that Sidaris is a serious filmmaker and not just ...\n",
              "14  1317  ...  if.... is the cinematic equivalent of Sgt. Pepper's: Revered by baby boomers as the pinnacle of creation, and viewed as rather a silly bit of business by preceding and subsequent generations. Now ...\n",
              "15  1218  ...  This early B entry into the patriotic category slapped a gorgeous young Gene Tierney on the ads and posters, but you have to wait a good time before you glimpse her, riding in a Hollywoodized came...\n",
              "16  1710  ...  I think Phillip Kaufman read the cliff's Notes version of the Kundera novel and then set about making this film. Okay, of course it won't have the punch of the original. Kundera's novels are great...\n",
              "17  1787  ...  My roommate had bought this documentary and invited me to watch it with her. She's from China and only heard so much about 9/11 and wanted to know the cold hard truth and she wanted me to tell her...\n",
              "18  1995  ...  'Maladolescenza' has the air of a dark fairy tale, with its child protagonists, forest setting, and the discovery of a castle's ruins. Yet at its core, the film is essentially an unusual psychosex...\n",
              "19  1471  ...  But the opposite, sorry bud, i completely understand how you can be dragged into a film because you relate to the subject ( and you have). This film is terrible, the main character would give any ...\n",
              "\n",
              "[20 rows x 5 columns]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "analyze(big_classifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxwwblfh9pqf"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Dg2J1BLgatMP"
      ],
      "name": "HW2",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}