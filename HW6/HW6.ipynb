{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4f183a"
      },
      "source": [
        "## Homework 6\n",
        "\n",
        "In this homework, you will be working with WordNet synsets and exploring methods to align new words (not in WordNet) with an existing synset."
      ],
      "id": "0a4f183a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0adaa568"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import math\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import wordnet as wn\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "!wget https://people.ischool.berkeley.edu/~dbamman/glove.6B.100d.100K.txt\n",
        "!pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "id": "0adaa568"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c188261"
      },
      "source": [
        "# Preliminaries: WordNet"
      ],
      "id": "9c188261"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd0cba00"
      },
      "source": [
        "NLTK provides a great interface to the WordNet ontology.  Remember that core unit within WordNet is the **synset** (a category of near-synonyms).  A word (like \"blue\") can appear in many different synsets, each corresponding to a distinct *sense* of that word."
      ],
      "id": "dd0cba00"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "852cf585"
      },
      "outputs": [],
      "source": [
        "# get all of the synsets that a specific word belongs to; print their definitions\n",
        "\n",
        "synsets=wn.synsets('blue')\n",
        "for synset in synsets:\n",
        "    print (synset, synset.definition())"
      ],
      "id": "852cf585"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28f268c3"
      },
      "source": [
        "Any given synset will likewise contain multiple different words (all near-synonyms of each other)."
      ],
      "id": "28f268c3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f809838"
      },
      "outputs": [],
      "source": [
        "# get all of the words/phrase in a given synset\n",
        "\n",
        "for lemma in wn.synset(\"gloomy.s.02\").lemmas():\n",
        "    print (lemma.name())"
      ],
      "id": "5f809838"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7a2d1e5"
      },
      "source": [
        "Remember also that one of the powerful things about WordNet is that it places synsets within a hierarchical structure; a given synset has both **hypernyms** (other synsets that it is a subclass of) and **hyponyms** (other synsets that are subclasses of it)."
      ],
      "id": "a7a2d1e5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53b8ff2f"
      },
      "outputs": [],
      "source": [
        "# Functions from http://www.nltk.org/howto/wordnet.html to get *all* of a synset's hyponym/hypernyms\n",
        "\n",
        "hypo = lambda s: s.hyponyms()\n",
        "hyper = lambda s: s.hypernyms()"
      ],
      "id": "53b8ff2f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "033786e8"
      },
      "source": [
        "Find all of the synsets that are hyponyms of the target synset (descendents in the WordNet hierarchy)"
      ],
      "id": "033786e8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15a5988f"
      },
      "outputs": [],
      "source": [
        "list(wn.synset(\"blue.n.01\").closure(hypo))"
      ],
      "id": "15a5988f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84b305eb"
      },
      "source": [
        "Find all of the synsets that are hyperyms (ancestors up the tree) of the target synset"
      ],
      "id": "84b305eb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2c22cece"
      },
      "outputs": [],
      "source": [
        "list(wn.synset(\"blue.n.01\").closure(hyper))"
      ],
      "id": "2c22cece"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ac1001c"
      },
      "source": [
        "Here's how you can access all of the synsets in WordNet through NLTK (though note executing this may take a while, so it's commented out)."
      ],
      "id": "5ac1001c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "449d8b45"
      },
      "outputs": [],
      "source": [
        "#for idx, synset in enumerate(wn.all_synsets()):\n",
        "#   print(idx, synset)\n",
        "#   if (idx > 10): break"
      ],
      "id": "449d8b45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b4cc005"
      },
      "source": [
        "# Homework"
      ],
      "id": "0b4cc005"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cbe0f1a"
      },
      "source": [
        "WordNet is a great resource, but one of its downsides is *coverage* -- many of the words in our vocabulay aren't in WordNet, but could conceivably be placed within existing synsets within it.  Your task for this homework is to develop two methods to finding the closest synset for a given new word from Urban Dictionary."
      ],
      "id": "3cbe0f1a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26301aaf"
      },
      "source": [
        "For the scope of this homework, we're only going to pretend that WordNet only has 12 different synsets within it (though feel free to use the `wn.all_synsets` function above if you wanted to explore running it on all of WordNet)."
      ],
      "id": "26301aaf"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2f0c3485"
      },
      "outputs": [],
      "source": [
        "target_synsets=['spread.n.01', 'formidable.s.01', 'coziness.n.01', 'mutation.n.02', 'kernel.n.03', 'faineant.s.01', 'fund-raise.v.01', 'orientation.n.06', 'inappropriate.a.01', 'stranger.n.02', 'plausibility.n.01', 'sever.v.01']"
      ],
      "id": "2f0c3485"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81f2c025"
      },
      "outputs": [],
      "source": [
        "for synset in target_synsets:\n",
        "    wn_synset=wn.synset(synset)\n",
        "    print(wn_synset)\n",
        "    print(\"\\tDefinition:\", wn_synset.definition())"
      ],
      "id": "81f2c025"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cff98e02"
      },
      "source": [
        "Here are the words that do not exist in WordNet now but that we want to add.  Each element of the tuple is (word, definition)."
      ],
      "id": "cff98e02"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20eae7c7"
      },
      "outputs": [],
      "source": [
        "urban_dictionary_terms: List[Tuple[str, str]] = [\n",
        "    (\"Crowdfunding\", \"the practice of obtaining needed funding (as for a new business) by soliciting contributions from a large number of people especially from the online community\"), \n",
        "    (\"Hygge\", \"a cozy quality that makes a person feel content and comfortable\"), \n",
        "    (\"biohacking\", \"biological experimentation (as by gene editing or the use of drugs or implants) done to improve the qualities or capabilities of living organisms especially by individuals and groups working outside a traditional medical or scientific research environment\"), \n",
        "    (\"TL;DR\", \"a briefly expressed main point or key message that summarizes a longer discussion or explanation\"), \n",
        "    (\"Hellacious\", \"Exceptionally powerful or violent; remarkably good; extremely difficult; extraordinarily large\"), \n",
        "    (\"Unfriend\", \"To remove from one's list of friends (e.g. on a social networking website)\"), \n",
        "    (\"Infodemic\", \"A wide and rapid spread of misinformation through various media, namely social media\"),\n",
        "    (\"Onboarding\", \"The act or process of orienting and training a new employee\"), \n",
        "    (\"Truthiness\", \"something that seems true but isnâ€™t backed up by evidence\"), \n",
        "    (\"Amotivational\", \"Relating to, or characterised by, a lack of motivation\"), \n",
        "    (\"NSFW\", \"Not Safe For Work. Used to describe Internet content generally inappropriate for the typical workplace, i.e., would not be acceptable in the presence of your boss and colleagues\"),\n",
        "    (\"Rando\", \"a person who is not known or recognizable or whose appearance (as in a conversation or narrative) seems unprompted or unwelcome\")\n",
        "]"
      ],
      "id": "20eae7c7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "890ffff6"
      },
      "source": [
        "Your task here is to develop two different methods for finding the best matching synset.\n",
        "1. Find the WordNet synset with the highest cosine similarity between the average GloVe embeddings of its synset definition and the average GloVe embeddings of the new word definition.\n",
        "2. Find the WordNet synset with the highest cosine similarity between the sentence embedding its synset definition and the sentence embedding of the new word definition."
      ],
      "id": "890ffff6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afcfbfde"
      },
      "source": [
        "Here is some code for reading in Glove embeddings:\n"
      ],
      "id": "afcfbfde"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6204372b"
      },
      "outputs": [],
      "source": [
        "def read_vectors(filename: str):\n",
        "    vocab_map={}\n",
        "    embeddings=[]\n",
        "    with(open(filename, encoding=\"utf-8\")) as file:\n",
        "        for idx, line in enumerate(file):\n",
        "            cols=line.rstrip().split(\" \")\n",
        "            word=cols[0]\n",
        "            embedding=cols[1:]\n",
        "\n",
        "            embeddings.append(embedding)\n",
        "            vocab_map[word]=idx\n",
        "    \n",
        "    return vocab_map, np.array(embeddings, dtype=\"float\")"
      ],
      "id": "6204372b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90c21c48"
      },
      "outputs": [],
      "source": [
        "glove_vocab_map, glove_embeddings=read_vectors(\"glove.6B.100d.100K.txt\")"
      ],
      "id": "90c21c48"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f398eaec"
      },
      "source": [
        "Here is some code for loading the sentence transformer package:"
      ],
      "id": "f398eaec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82797d7d"
      },
      "outputs": [],
      "source": [
        "sentence_model = SentenceTransformer('sentence-transformers/all-distilroberta-v1')\n",
        "\n",
        "sentence_vector=sentence_model.encode(\"this is a sentence\")\n",
        "print(sentence_vector.shape)\n"
      ],
      "id": "82797d7d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's an implementation of cosine similarity that you will find useful."
      ],
      "metadata": {
        "id": "CEciw_iyhWZ3"
      },
      "id": "CEciw_iyhWZ3"
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(one, two):\n",
        "    return np.dot(one, two) / (np.linalg.norm(one) * np.linalg.norm(two))"
      ],
      "metadata": {
        "id": "uQU8eoHKhJ4h"
      },
      "id": "uQU8eoHKhJ4h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "102ce6a9"
      },
      "source": [
        "#### Q1. Implement the first method as `method_one` below.\n",
        "\n",
        "As mentioned above, you should compute the average GloVe embedding of the UD word definition and use cosine similarity to compare it with the average GloVe embedding of the synset definitions. For each UD word, choose the definition that maximizes the cosine similarity with its definition. Here are some things you need to do when calculating the average GloVe embedding of a sentence:\n",
        "- Use `nltk.word_tokenize()` to tokenize the sentence.\n",
        "- Treat everything as lowercase.\n",
        "- Skip any tokens which don't appear in the GloVe vocabulary.\n",
        "- Calculate the average value of the embedding vectors, which will be another vector of the same shape.\n",
        "\n",
        "Your function should return a dictionary mapping each urban dictionary term to a WordNet synset ID, e.g.:\n",
        "\n",
        "`{\n",
        " \"adore\" : \"love.v.01\",\n",
        " \"dripping\" : \"stylish.a.01\"    \n",
        " }`\n",
        "\n",
        " Please make sure that any helper functions that you use are defined *within* `method_one`! That will help us extract your code more easily with the autograder."
      ],
      "id": "102ce6a9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5737fbe2"
      },
      "outputs": [],
      "source": [
        "def method_one(urban_dictionary_terms: List[Tuple[str, str]], target_synsets: List[str]):\n",
        "    \"\"\"\n",
        "    Method 1: an algorithm based on GloVe embeddings that maps each urban dictionary term to a synset ID.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    urban_dictionary_terms : List[Tuple[str, str]]\n",
        "        a list of string 2-tuples where the first elements are words, second elements are definitions.\n",
        "    target_synsets : List[str]\n",
        "        a list of synset IDs that the words should be classified into.\n",
        "        You can call `wn.synset(\"<synset ID>\")` to get the synset object.\n",
        "    \n",
        "    Returns\n",
        "    --------\n",
        "    A dictionary mapping each urban dictionary term to a WordNet synet ID, e.g.\n",
        "    `{\"adore\" : \"love.v.01\", \"dripping\" : \"stylish.a.01\"}`\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    # Your code\n",
        "\n",
        "    pass\n"
      ],
      "id": "5737fbe2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b69452b2"
      },
      "outputs": [],
      "source": [
        "method_one_results=method_one(urban_dictionary_terms, target_synsets)\n",
        "method_one_results"
      ],
      "id": "b69452b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b862967"
      },
      "source": [
        "#### Q2. Implement your second method as `method_two` below.\n",
        "\n",
        "In this function, you should compute the cosine similarity between the sentence embedding of the UD word definition and those of the synsets, then for each UD word, choose the synset with the highest cosine similarity. For consistency, use the sentence transformer model called `sentence-transformers/all-distilroberta-v1`.\n",
        "\n",
        "Your function must also return a dictionary mapping each urban dictionary term to a WordNet synset ID, e.g.:\n",
        "\n",
        "`{\n",
        " \"adore\" : \"love.v.01\",\n",
        " \"dripping\" : \"stylish.a.01\"    \n",
        " }`\n",
        "\n",
        "As before, please make sure that any helper functions that you use are defined *within* `method_two`! That will help us extract your code more easily with the autograder."
      ],
      "id": "0b862967"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d2b70a3"
      },
      "outputs": [],
      "source": [
        "def method_two(urban_dictionary_terms: List[Tuple[str, str]], target_synsets: List[str]):\n",
        "    \"\"\"\n",
        "    Method 2: an algorithm based on sentence embeddings that maps each urban dictionary term to a synset ID.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    urban_dictionary_terms : List[Tuple[str, str]]\n",
        "        a list of string 2-tuples where the first elements are words, second elements are definitions.\n",
        "    target_synsets : List[str]\n",
        "        a list of synset IDs that the words should be classified into.\n",
        "        You can call `wn.synset(\"<synset ID>\")` to get the synset object.\n",
        "    \n",
        "    Returns\n",
        "    --------\n",
        "    A dictionary mapping each urban dictionary term to a WordNet synet ID, e.g.\n",
        "    `{\"adore\" : \"love.v.01\", \"dripping\" : \"stylish.a.01\"}`\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    # Your code\n",
        "\n",
        "    pass\n"
      ],
      "id": "7d2b70a3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "285b848a"
      },
      "outputs": [],
      "source": [
        "method_two_results=method_two(urban_dictionary_terms, target_synsets)\n",
        "method_two_results"
      ],
      "id": "285b848a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f329391"
      },
      "source": [
        "#### Q3: Define an evaluation metric (accuracy).  \n",
        "\n",
        "Throughout this semester we've stressed how critical evaluation is for any NLP method.  Implement a function `accuracy` that assesses quality of the dictionaries you return from `method_one` and `method_two`.  This accuracy function should return a single real number (the accuracy), and its input parameters are a prediction dict (the output of your model) and a truth dict (which you will need to create based on your own judgement). Make sure that the accuracies you calculate for the two methods match what you expect. "
      ],
      "id": "5f329391"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "938b9fcd"
      },
      "outputs": [],
      "source": [
        "def accuracy(prediction: Dict[str, str], truth: Dict[str, str]) -> float:\n",
        "    pass"
      ],
      "id": "938b9fcd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "766150ed"
      },
      "outputs": [],
      "source": [
        "truth = ..."
      ],
      "id": "766150ed"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ea46c6b"
      },
      "outputs": [],
      "source": [
        "print(accuracy(method_one_results, truth))"
      ],
      "id": "9ea46c6b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0135985"
      },
      "outputs": [],
      "source": [
        "print(accuracy(method_two_results, truth))"
      ],
      "id": "c0135985"
    },
    {
      "cell_type": "markdown",
      "source": [
        "That concludes homework 6! To submit, just upload this .ipynb file to Gradescope."
      ],
      "metadata": {
        "id": "8eZv8eZJ8sfE"
      },
      "id": "8eZv8eZJ8sfE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Q4 (optional)\n",
        "\n",
        "Use the two methods you've defined to find the best-matching synset within the **entire** WordNet. Do the results make sense? Is one method consistently better than the other? Why?\n",
        "\n",
        "Here's a reminder of how to iterate through all synsets:"
      ],
      "metadata": {
        "id": "djEZMMFT7j5G"
      },
      "id": "djEZMMFT7j5G"
    },
    {
      "cell_type": "code",
      "source": [
        "for idx, synset in enumerate(wn.all_synsets()):\n",
        "   print(idx, synset)\n",
        "   if (idx > 10): break"
      ],
      "metadata": {
        "id": "6M_gxGMo7g2T"
      },
      "id": "6M_gxGMo7g2T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "z7Jpm8zN726O"
      },
      "id": "z7Jpm8zN726O",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "HW6 student.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}